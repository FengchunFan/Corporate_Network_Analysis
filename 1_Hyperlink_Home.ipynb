{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl22J5zZXMJQ"
   },
   "source": [
    "# **Program to extract external hyperlinks on the target Home URL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi8AKz_AYMdI"
   },
   "source": [
    "# Extract external hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10098,
     "status": "ok",
     "timestamp": 1743037156682,
     "user": {
      "displayName": "Fengchun Fan",
      "userId": "16607154835906475343"
     },
     "user_tz": 420
    },
    "id": "00_bR7Q2XdMw",
    "outputId": "75871baa-1bec-48f5-b6c9-db3aecb1d5ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "# Install requests for web service\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2388,
     "status": "ok",
     "timestamp": 1743037159073,
     "user": {
      "displayName": "Fengchun Fan",
      "userId": "16607154835906475343"
     },
     "user_tz": 420
    },
    "id": "bxjd_A96XZ6C"
   },
   "outputs": [],
   "source": [
    "# Library needed\n",
    "# Regular Python Data analyze library\n",
    "import pandas as pd\n",
    "\n",
    "# Web related library\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1743037159085,
     "user": {
      "displayName": "Fengchun Fan",
      "userId": "16607154835906475343"
     },
     "user_tz": 420
    },
    "id": "tdVrjkKtXJ_u"
   },
   "outputs": [],
   "source": [
    "# Replace the home_url here\n",
    "Home_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1743037159107,
     "user": {
      "displayName": "Fengchun Fan",
      "userId": "16607154835906475343"
     },
     "user_tz": 420
    },
    "id": "mY4Qze7XYVGb"
   },
   "outputs": [],
   "source": [
    "# User Agent to prevent being identified as a bot\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1743037159175,
     "user": {
      "displayName": "Fengchun Fan",
      "userId": "16607154835906475343"
     },
     "user_tz": 420
    },
    "id": "ZZqgGxkJHfx6"
   },
   "outputs": [],
   "source": [
    "MAX_PAGES = 10000\n",
    "# Google Colab Memory Exceed\n",
    "MAX_QUEUE = 20000\n",
    "MAX_DEPTH = 5  # Limit depth to 3\n",
    "\n",
    "# Function to extract all external links\n",
    "# From the starting homepage\n",
    "def extract_links(start_url):\n",
    "  to_visit = [(start_url, 0)]  # Queue of (url, depth)\n",
    "\n",
    "  # While queue is not empty\n",
    "  while to_visit:\n",
    "    # extract homepage link and current depth\n",
    "    url, depth = to_visit.pop(0)\n",
    "\n",
    "    # Check if we've hit the page limit\n",
    "    if len(visited_urls) >= MAX_PAGES or len(to_visit) >= MAX_QUEUE:\n",
    "      break\n",
    "\n",
    "    # Skip if the URL has already been visited\n",
    "    if url in visited_urls or depth > MAX_DEPTH:\n",
    "      continue\n",
    "\n",
    "    # First attempt with User-Agent header\n",
    "    try:\n",
    "      response = requests.get(url, headers=headers, timeout=10)\n",
    "      response.encoding = 'utf-8'  # explicitly set the encoding\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "      print(f\"Failed to access {url} with User-Agent: {e}, attempting without User-Agent\")\n",
    "      # Second attempt without User-Agent\n",
    "      try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.encoding = 'utf-8'  # explicitly set the encoding\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      except requests.RequestException as e:\n",
    "        print(f\"Failed to access {url} without User-Agent: {e}, skipping this URL\")\n",
    "        continue\n",
    "\n",
    "    # Mark the URL as visited\n",
    "    visited_urls.add(url)\n",
    "    # print(f\"Visiting Internal Url (Depth {depth}): {url}\")\n",
    "\n",
    "    # Get the base domain and path of the URL\n",
    "    base_domain = urlparse(url).netloc\n",
    "    base_path = urlparse(url).path\n",
    "\n",
    "    # Find all links on the page\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Go through all links in the list\n",
    "    for link in links:\n",
    "      href = link.get('href')\n",
    "      if href:\n",
    "        full_url = urljoin(url, href)\n",
    "        parsed_url = urlparse(full_url)\n",
    "\n",
    "        # If it's an external link, add to external_links set\n",
    "        if parsed_url.netloc != base_domain:\n",
    "          external_links.add(full_url)\n",
    "        else:\n",
    "          # If it's an internal link, and not yet visited, add to the queue\n",
    "          # Make sure the base domain and base path both matches\n",
    "          if parsed_url.netloc == base_domain and parsed_url.path.startswith(base_path):\n",
    "            if full_url not in visited_urls and not full_url.endswith((\".pdf\", \".jpg\", \".png\", \".gif\", \".zip\")):\n",
    "              # Ensure the full_url is not in the to_visit queue with any depth\n",
    "              if not any(url == full_url for url, _ in to_visit):\n",
    "                # Check if the URL contains \"sponsor\" or \"partner\"\n",
    "                if \"Sponsor\" in full_url or \"sponsor\" in full_url or \"Partner\" in full_url or \"partner\" in full_url or \"Funder\" in full_url or \"funder\" in full_url or \"Donor\" in full_url or \"donor\" in full_url:\n",
    "                    to_visit.insert(0, (full_url, depth + 1))  # Add to the front of the queue\n",
    "                else:\n",
    "                    to_visit.append((full_url, depth + 1))  # Add to the end of the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 39352,
     "status": "ok",
     "timestamp": 1743037198534,
     "user": {
      "displayName": "Fengchun Fan",
      "userId": "16607154835906475343"
     },
     "user_tz": 420
    },
    "id": "fAy65NI1Y4Hr"
   },
   "outputs": [],
   "source": [
    "# Click Run, and you should be able to see the internal links the cralwer visting\n",
    "# The program running time highly depends on the website size\n",
    "# You can run this multiple times to ensure all corners of the homepage has been visited\n",
    "\n",
    "# Track visited internal URLs and external links\n",
    "visited_urls = set()\n",
    "external_links = set()\n",
    "# Get all the external links on the web of this sponsor\n",
    "extract_links(Home_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1743037198536,
     "user": {
      "displayName": "Fengchun Fan",
      "userId": "16607154835906475343"
     },
     "user_tz": 420
    },
    "id": "QDqXNOY6ZI2l",
    "outputId": "467d7d0e-ef1d-45bf-a1f6-faddcaf71d84"
   },
   "outputs": [],
   "source": [
    "# View the external links\n",
    "# The ones ends with \".com\", \".org\" etc are usually more interesting\n",
    "external_links"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOwnycjn5k2DtY6rKPhN8JP",
   "collapsed_sections": [
    "TJdu6el5YC5V",
    "STgnCEoNandN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
